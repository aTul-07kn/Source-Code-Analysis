{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb452fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iuhgfc\n"
     ]
    }
   ],
   "source": [
    "print(\"iuhgfc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4146fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Gai-spots\\\\Source-Code-Analysis\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a678f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29828202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/aTul-07kn/Medi-Chatbot-Medicrolina\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3628a70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (0.3.61)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (0.5.14)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (0.3.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: certifi in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\gai-spots\\source-code-analysis\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02689c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211e91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfad840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\ test.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request \\nfrom src.helper import *\\nfrom src.prompt import system_prompt\\nfrom langchain_pinecone import PineconeVectorStore\\nimport os\\nfrom dotenv import load_dotenv \\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\n\\nload_dotenv()\\n\\napp = Flask(__name__)\\n\\nPINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\\nNVIDIA_API_KEY=os.getenv(\"NVIDIA_API_KEY\")\\nos.environ[\"NVIDIA_API_KEY\"]=NVIDIA_API_KEY\\n\\nembeddings=download_hugging_face_embeddings()\\nindex_name=\"medi-chatbot-index\"\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(\\n    search_type=\"similarity\",\\n    search_kwargs={\"k\": 3},\\n)\\n\\nllm = ChatNVIDIA(model=\"nvidia/llama-3.1-nemotron-70b-instruct\", \\n                 max_tokens=500, \\n                 temperature=0.7, \\n                 verbose=True)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nstuff_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, stuff_chain)\\n\\n# @app.route(\\'/\\')\\n# def hello():\\n#     return \\'Hello, World!\\'\\n\\n# @app.route(\"/help\", methods=[\\'POST\\'])\\n# def help():\\n#     return \"helping myself\"\\n\\n# if __name__ == \\'__main__\\':\\n#     # Enables debug mode (auto–reload + better error messages)\\n#     app.run(port=8000, debug=True)\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\nif __name__ == \\'__main__\\':\\n    app.run(port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Medicrolina Chatbot',\\n    version= '0.0.0',\\n    author= 'Atul Kumar Nayak',\\n    author_email= 'atulnayak7869@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_data\\nfrom src.helper import split_text\\nfrom src.helper import download_hugging_face_embeddings\\nimport os\\nfrom dotenv import load_dotenv \\n# Import the Pinecone library\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\n\\nload_dotenv()\\nPINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\\n\\nextracted_docs=load_data(\"Data/\")\\nchunks=split_text(extracted_docs)\\nembeddings=download_hugging_face_embeddings()\\n\\n\\n# Initialize a Pinecone client with your API key\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\n# Create a dense index with integrated embedding\\nindex_name = \"medi-chatbot-index\"\\nif not pc.has_index(index_name):\\n    pc.create_index(\\n        name=index_name,\\n        dimension=384, # Replace with your model dimensions\\n        metric=\"cosine\", # Replace with your model metric\\n        spec=ServerlessSpec(\\n            cloud=\"aws\",\\n            region=\"us-east-1\"\\n        )\\n    )\\n    \\nvectorstore_from_docs = PineconeVectorStore.from_documents(\\n        documents=chunks,\\n        index_name=index_name,\\n        embedding=embeddings\\n    )'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\ndef load_data(data):\\n    loader=DirectoryLoader(\\n        data, \\n        glob=\"*.pdf\", \\n        loader_cls=PyPDFLoader,\\n        show_progress=True)\\n\\n    docs=loader.load()\\n    return docs\\n\\n\\ndef split_text(extracted_docs):\\n    splitter=RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=40)\\n    split_chunks=splitter.split_documents(extracted_docs)\\n    return split_chunks\\n\\n\\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name=\\'sentence-transformers/all-MiniLM-L6-v2\\')\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b938c656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8dde189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a94f3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5294667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request \\nfrom src.helper import *\\nfrom src.prompt import system_prompt\\nfrom langchain_pinecone import PineconeVectorStore\\nimport os\\nfrom dotenv import load_dotenv \\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\n\\napp = Flask(__name__)\\n\\nPINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\\nNVIDIA_API_KEY=os.getenv(\"NVIDIA_API_KEY\")\\nos.environ[\"NVIDIA_API_KEY\"]=NVIDIA_API_KEY\\n\\nembeddings=download_hugging_face_embeddings()\\nindex_name=\"medi-chatbot-index\"\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(\\n    search_type=\"similarity\",\\n    search_kwargs={\"k\": 3},\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='llm = ChatNVIDIA(model=\"nvidia/llama-3.1-nemotron-70b-instruct\", \\n                 max_tokens=500, \\n                 temperature=0.7, \\n                 verbose=True)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nstuff_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, stuff_chain)\\n\\n# @app.route(\\'/\\')\\n# def hello():\\n#     return \\'Hello, World!\\''),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='# @app.route(\"/help\", methods=[\\'POST\\'])\\n# def help():\\n#     return \"helping myself\"\\n\\n# if __name__ == \\'__main__\\':\\n#     # Enables debug mode (auto–reload + better error messages)\\n#     app.run(port=8000, debug=True)\\n\\n\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\nif __name__ == \\'__main__\\':\\n    app.run(port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Medicrolina Chatbot',\\n    version= '0.0.0',\\n    author= 'Atul Kumar Nayak',\\n    author_email= 'atulnayak7869@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_data\\nfrom src.helper import split_text\\nfrom src.helper import download_hugging_face_embeddings\\nimport os\\nfrom dotenv import load_dotenv \\n# Import the Pinecone library\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\n\\nload_dotenv()\\nPINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\\n\\nextracted_docs=load_data(\"Data/\")\\nchunks=split_text(extracted_docs)\\nembeddings=download_hugging_face_embeddings()'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Initialize a Pinecone client with your API key\\npc = Pinecone(api_key=PINECONE_API_KEY)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Create a dense index with integrated embedding\\nindex_name = \"medi-chatbot-index\"\\nif not pc.has_index(index_name):\\n    pc.create_index(\\n        name=index_name,\\n        dimension=384, # Replace with your model dimensions\\n        metric=\"cosine\", # Replace with your model metric\\n        spec=ServerlessSpec(\\n            cloud=\"aws\",\\n            region=\"us-east-1\"\\n        )\\n    )\\n    \\nvectorstore_from_docs = PineconeVectorStore.from_documents(\\n        documents=chunks,'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='documents=chunks,\\n        index_name=index_name,\\n        embedding=embeddings\\n    )'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\ndef load_data(data):\\n    loader=DirectoryLoader(\\n        data, \\n        glob=\"*.pdf\", \\n        loader_cls=PyPDFLoader,\\n        show_progress=True)\\n\\n    docs=loader.load()\\n    return docs'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def split_text(extracted_docs):\\n    splitter=RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=40)\\n    split_chunks=splitter.split_documents(extracted_docs)\\n    return split_chunks\\n\\n\\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\\n    return embeddings\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce52a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30151fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U --quiet langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68bfaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NVIDIA_API_KEY=os.getenv(\"NVIDIA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54545ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NVIDIA_API_KEY\"]=NVIDIA_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a4b0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "embedder = NVIDIAEmbeddings(\n",
    "  model=\"NV-Embed-QA\",  \n",
    "  truncate=\"NONE\", \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "372442cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(texts, embedding=embedder, persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bb1c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATUL\\AppData\\Local\\Temp\\ipykernel_19988\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ede3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd=embedder.embed_query(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c2a8030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be387c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(\n",
    "  model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "  temperature=0.6,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7264043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='nvidia/llama-3.1-nemotron-70b-instruct', temperature=0.6, top_p=1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2380bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATUL\\AppData\\Local\\Temp\\ipykernel_19988\\2984388413.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fc29b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6b98544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATUL\\AppData\\Local\\Temp\\ipykernel_19988\\2264811874.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here's what we can infer about the `download_hugging_face_embeddings` function:\n",
      "\n",
      "**Function Signature:**\n",
      "```python\n",
      "def download_hugging_face_embeddings():\n",
      "```\n",
      "**Return Value:**\n",
      "The function returns an object named `embeddings`.\n",
      "\n",
      "**Implementation Details:**\n",
      "Inside the function, it creates an instance of `HuggingFaceEmbeddings` from the `langchain_huggingface.embeddings` module, specifying a particular `model_name`:\n",
      "```python\n",
      "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "```\n",
      "**Inference:**\n",
      "Given this information, we can infer that:\n",
      "\n",
      "1. **Purpose:** The primary purpose of `download_hugging_face_embeddings` is to fetch and return a specific Hugging Face embedding model.\n",
      "2. **Embedding Model:** The function downloads and returns the **`all-MiniLM-L6-v2`** model from the **`sentence-transformers`** repository on Hugging Face.\n",
      "3. **Return Type:** Although the exact type is not explicitly stated, the return value (`embeddings`) is likely an instance of `HuggingFaceEmbeddings`, which probably provides methods for generating embeddings from text inputs.\n",
      "\n",
      "**Example Usage:**\n",
      "In the provided context, the function is called to obtain the `embeddings` object, which is then used to:\n",
      "```python\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "docsearch = PineconeVectorStore.from_existing_index(\n",
      "    index_name=index_name,\n",
      "    embedding=embeddings\n",
      ")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = \"what is download_hugging_face_embeddings funtion?\"\n",
    "\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7048c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here is a concise answer to the standalone question:\n",
      "\n",
      "The **system_prompt** says:\n",
      "* You are an assistant for question-answering tasks.\n",
      "* Use the following pieces of retrieved context to answer the question.\n",
      "* If you don't know the answer, say that you don't know.\n",
      "* Use three sentences maximum and keep the answer concise.\n"
     ]
    }
   ],
   "source": [
    "result = qa(\"what does the **system_prompt** say\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a6b41d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided **system_prompt**, here's a concise answer with suggestions for improvement:\n",
      "\n",
      "**Improvement Suggestions for **system_prompt****\n",
      "\n",
      "1. **Specify Context Scope**: Clarify what is meant by \"the following pieces of retrieved context\" (e.g., are they always from a specific source, format, or domain?).\n",
      "2. **Define Conciseness Boundaries**: Elaborate on \"keep the answer concise\" by providing a character/token limit or a specific response format (e.g., bullet points, short paragraphs).\n",
      "3. **Consider Adding a Tone or Style Guideline**: To ensure consistency, suggest a tone (e.g., formal, friendly) or style (e.g., objective, persuasive) for the responses.\n",
      "\n",
      "**Revised System Prompt Example (incorporating suggestions):**\n",
      "```\n",
      "\"You are an assistant for question-answering tasks. \n",
      "Please respond using the provided context below, \n",
      "if insufficient, say 'I don't know'. \n",
      "Keep answers within 250 characters, \n",
      "using short paragraphs or bullet points, \n",
      "and maintain a neutral, informative tone.\n",
      "\n",
      "\\n\\n\n",
      "{context}\"\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "result = qa(\"Can we improve this prompt, improve it.\")\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
